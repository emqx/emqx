emqx_connector_aggregator_schema {

container.label:
"""Container for aggregated events"""
container.desc:
"""Settings governing the file format of an upload containing aggregated events."""

container_csv.label:
"""CSV container"""
container_csv.desc:
"""Records (events) will be aggregated and uploaded as a CSV file."""

container_csv_column_order.label:
"""CSV column order"""
container_csv_column_order.desc:
"""Event fields that will be ordered first as columns in the resulting CSV file.<br/>
Regardless of this setting, resulting CSV will contain all the fields of aggregated events, but all the columns not explicitly mentioned here will be ordered after the ones listed here in the lexicographical order."""

container_json_lines.label:
"""JSON Lines container"""
container_json_lines.desc:
  """Records (events) will be aggregated and uploaded as a [JSON Lines](https://jsonlines.org/) file."""

  container_parquet {
    label: """Parquet Data File"""
    desc:
    """Use [Parquet](https://parquet.apache.org/docs/) for encoding data files."""
  }

  parquet_avro_schema {
    label: """Schema (Avro)"""
    desc:
    """The schema to be used for the Parquet files, in Avro format."""
  }

  container_parquet_max_row_group_bytes {
    label: """Parquet Max Row Group Bytes"""
    desc:
    """The size above which row groups are flushed.  The actual triggering of row group flushes uses estimates of the buffered data sizes, since the actual output may differ in size from the raw input data size."""
  }

  parquet_default_compression {
    label: """Parquet Default Compression"""
    desc:
    """The default algorithm to use for compressing data pages in Parquet row groups."""
  }

}
