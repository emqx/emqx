emqx_ds_schema {

messages.label: "MQTT message storage"
messages.desc:
  """~
  Configuration related to the durable storage of MQTT messages.~"""

builtin_raft.label: "Builtin backend with Raft replication"
builtin_raft.desc:
  """~
  Builtin storage backend utilizing embedded RocksDB key-value store.~"""

builtin_local.label: "Builtin backend"
builtin_local.desc:
  """~
  Builtin storage backend utilizing embedded RocksDB key-value store.
  This backend doesn't support clustering.~"""

backend_type.label: "Backend type"
backend_type.desc:
  """~
  Backend type.~"""

builtin_data_dir.label: "Database location"
builtin_data_dir.desc:
  """~
  File system directory where the database is located.

  By default, it is equal to `node.data_dir`.~"""

builtin_n_shards.label: "Number of shards"
builtin_n_shards.desc:
  """~
  The built-in durable storage partitions data into shards.
  This configuration parameter defines the number of shards.
  Please note that it takes effect only during the initialization of the durable storage database.
  Changing this configuration parameter after the database has been already created won't take any effect.~"""

builtin_raft_replication_factor.label: "Replication factor"
builtin_raft_replication_factor.desc:
  """~
  Number of identical replicas each shard should have.
  Increasing this number improves durability and availability at the expense of greater resource consumption.
  Quorum of replicas is needed to be healthy for the replication to work, hence an odd number of replicas is a good pick in general.
  Please note that it takes effect only during the initialization of the durable storage database.
  Changing this configuration parameter after the database has been already created won't take any effect.~"""

builtin_raft_n_sites.label: "Initial number of sites"
builtin_raft_n_sites.desc:
  """~
  Number of storage sites that need to share responsibility over the set of storage shards.
  In this context, sites are EMQX nodes with message durability enabled.
  Please note that it takes effect only during the initialization of the durable storage database.
  During this phase at least that many sites should come online to distribute shards between them, otherwise message storage will be unavailable until then.
  After the initialization is complete, sites may be offline, which will affect availability depending on the number of offline sites and replication factor.~"""

builtin_write_buffer.label: "Local write buffer"
builtin_write_buffer.desc:
  """~
  Configuration related to the buffering of messages sent from the local node to the shard leader.

  EMQX accumulates PUBLISH messages from the local clients in a write buffer before committing them to the durable storage.
  This helps to hide network latency between EMQX nodes and improves write throughput.~"""

builtin_write_buffer_max_items.label: "Max items"
builtin_write_buffer_max_items.desc:
  """~
  This configuration parameter defines maximum number of messages stored in the local write buffer.~"""

builtin_write_buffer_flush_interval.label: "Flush interval"
builtin_write_buffer_flush_interval.desc:
  """~
  Maximum linger time for the buffered messages.
  Local write buffer will be flushed _at least_ as often as `flush_interval`.

  Larger values of `flush_interval` may lead to higher throughput and better overall performance, but may increase end-to-end latency.~"""

builtin_layout.label: "Storage layout"
builtin_layout.desc:
  """~
  Storage layout is a method of arranging messages from various topics and clients on disc.

  Depending on the type of workload and the topic structure, different types of strategies for storing the data can be employed to maximize efficiency of reading messages from the durable storage.~"""

layout_builtin_wildcard_optimized.label: "Wildcard-optimized storage layout"
layout_builtin_wildcard_optimized.desc:
  """~
  _Wildcard-optimized_ layout is designed to maximize the throughput of wildcard subscriptions covering large numbers of topics.

  For example, it can handle scenarios where a very large number of clients publish data to the topics containing their client ID, such as: `sensor/%device-version%/%clientid%/temperature`, `sensor/%device-version%/%clientid%/pressure`, etc.
  This layout will automatically group such topics into a single stream, so a client subscribing to a topic filter containing wildcards (such as `sensor/+/+/temperature`) will be able to consume messages published by all devices as a single batch.

  This layout is efficient for non-wildcard subscriptions as well.~"""

layout_builtin_wildcard_optimized_type.label: "Layout type"
layout_builtin_wildcard_optimized_type.desc:
  """~
  Wildcard-optimized layout type.~"""

wildcard_optimized_epoch_bits.label: "Epoch bits"
wildcard_optimized_epoch_bits.desc:
  """~
  Wildcard-optimized layout partitions messages recorded at different times into "epochs".
  Reading messages from a single epoch can be done very efficiently, so larger epochs improve the throughput of subscribers, but may increase end-to-end latency.

  Time span covered by each epoch grows exponentially with the value of `epoch_bits`:

  - `epoch_bits = 1`: epoch time = 2 microseconds
  - `epoch_bits = 2`: 4 microseconds
  ...
  - `epoch_bits = 20`: ~1s
  ...~"""

layout_builtin_reference.label: "Reference layout"
layout_builtin_reference.desc:
  """~
  A simplistic layout type that stores all messages from all topics in chronological order in a single stream.

  Not recommended for production use.~"""

layout_builtin_reference_type.label: "Layout type"
layout_builtin_reference_type.desc: "Reference layout type."

optimistic_transaction.label: "Transaction"
optimistic_transaction.desc:
  """~
  Transaction settings for built-in durable storage backends.~"""

builtin_optimistic_transaction.label: "Transaction settings"
builtin_optimistic_transaction.desc: "Transaction settings."

otx_flush_interval.label: "Transaction flush interval"
otx_flush_interval.desc:
  """~
  Specifies the maximum time operations may linger in the buffer before they are committed to the storage.~"""

otx_idle_flush_interval.label: "Idle transaction flush interval"
otx_idle_flush_interval.desc:
  """~
  If shard doesn't receive new transactions within this period, the buffer is flushed early.~"""

otx_conflict_window.label: "Conflict tracking window"
otx_conflict_window.desc:
  """~
  Builtin durable storage backends track recent updates over a period of time known as the conflict tracking window.

  Transactions that started earlier than the beginning of the window are automatically rejected.
  So, effectively, this parameter limits the time the transactions can run.

  Higher values reduce the risk of rejecting transactions due to long run time, but may increase RAM demands.

  This value should be greater than the flush interval.~"""

}
