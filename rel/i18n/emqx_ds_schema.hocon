emqx_ds_schema {
  // Databases:

  messages {
    label: "MQTT Message Storage"
    desc:
      """~
      Configuration of the durable storage for MQTT messages.~"""
  }

  sessions {
    label: "MQTT Session Storage"
    desc:
      """~
      Configuration of the durable storage containing state and metadata of the durable sessions.~"""
  }

  timers {
     label: "Durable Timers Storage"
     desc:
       """~
       Configuration of the durable storage containing timers.~"""
  }

  shared_subs {
     label: "Durable Shared Subscriptions"
     desc:
       """~
       Configuration of the durable storage containing metadata and topic replay progress by shared subscriptions.~"""
  }

  mq_states {
      label: "Message Queue states"
      desc:
        """~
        Configuration of the durable storage containing Message Queue states."""
  }

  mq_messages {
      label: "Message Queue messages"
      desc:
        """~
        Configuration of the durable storage containing Message Queue messages."""
  }

  // Backends types:

  backend_type {
    label: "Backend type"
    desc:
      """~
      Durable storage backend.

      Please note that it's impossible to change the backend of existing durable storages without losing the data.~"""
  }

  builtin_raft {
    label: "Builtin Raft"
    desc:
      """~
      An embedded durable storage backend with support for data replication.

      This backend is intended for use in clustered EMQX deployments.
      It replicates data for each durable storage shard on several EMQX nodes.~"""
  }

  builtin_local {
    label: "Builtin Local"
    desc:
      """~
      An embedded durable storage backend without network capabilities.

      This backend doesn't support clustering, but it has lower overhead than `builtin_raft`.
      It's recommended for use in strictly single-node deployments.~"""
  }

  // Common options

  builtin_n_shards {
    label: "Number of Shards"
    desc:
      """~
      The durable storage partitions data into shards.
      This configuration parameter defines the number of shards.
      Please note that it takes effect only during the initialization of the durable storage database.
      Changing this configuration parameter after the database has been already created won't take any effect.~"""
  }

  db_group {
    label: "Database Group"
    desc:
      """~
      Assign the durable storage to a database group with the given name.
      Certain resources (such as disk usage quota) will be shared among all members of the group.

      Backend of the database group should match that of the storage.~"""
  }


  // Common options of builtin backends:

  builtin_data_dir {
    label: "Database Location"
    desc:
      """~
      Root directory for the data related to built-in storage backends.

      By default, it is equal to `node.data_dir/ds/`.~"""
  }

  // OTX

  optimistic_transaction {
    label: "Transaction"
    desc:
      """~
      Transaction settings for built-in durable storage backends.~"""
  }

  builtin_optimistic_transaction {
    label: "Transaction Settings"
    desc: "Transaction settings."
  }

  otx_flush_interval {
    label: "Transaction Flush Interval"
    desc:
      """~
      Specifies the maximum time operations may linger in the buffer before they are committed to the storage.
      Larger values of the parameter may improve the throughput while negatively affecting the latency.~"""
  }

  otx_idle_flush_interval {
    label: "Idle Transaction Flush Interval"
    desc:
      """~
      If shard doesn't receive new transactions within this period, the buffer is flushed early.~"""
  }

  otx_max_pending {
    label: "Maximum Pending Operations"
    desc:
      """~
      Specifies the maximum number of operations that can be held in the buffer.
      When this value is exceeded, the buffer is flushed regardless of other factors.~"""
  }

  otx_conflict_window {
    label: "Conflict Tracking Window"
    desc:
      """~
      Built-in durable storage backends track recent updates over a period of time known as the conflict tracking window.

      Transactions that started earlier than the beginning of the window are automatically rejected.
      So, effectively, this parameter limits the time the transactions can run.

      Higher values reduce the risk of rejecting transactions due to long run time, but may increase RAM demands.

      This value should be greater than the flush interval.~"""
  }

  // RocksDB tuning:

  rocksdb_options {
    label: "RocksDB Options"
    desc:
      """~
      Options concerning behavior of RocksDB databases used by the built-in durable storage backends.~"""
  }

  rocksdb_cache_size {
    label: "RocksDB Cache Size"
    desc:
      """~
      Size of the RocksDB block cache.

      Note that increasing this value may not necessarily reduce I/O, since the compressed data may reside in the OS's page cache, but it can reduce CPU load at the expense of memory usage.

      Each shard of the database has its own cache.~"""
  }

  rocksdb_write_buffer_size {
    label: "RocksDB Write Buffer Size"
    desc:
      """~
      This parameter is deprecated and will be removed in a future release.
      It has been replaced with `durable_storage.db_groups.write_buffer_size` parameter,
      which has similar semantics, but is an aggregate for all shards.
      .~"""
  }

  rocksdb_max_open_files {
    label: "Maximum Open Files"
    desc:
      """~
      Maximum number of open file descriptors per shard.~"""
  }

  // Raft-specific settings

  builtin_raft_replication_factor {
    label: "Replication Factor"
    desc:
      """~
      Number of identical replicas each shard should have.
      Increasing this number improves durability and availability at the expense of greater resource consumption.
      Quorum of replicas is needed to be healthy for the replication to work, hence an odd number of replicas is a good pick in general.
      Please note that it takes effect only during the initialization of the durable storage database.
      Changing this configuration parameter after the database has been already created won't take any effect.~"""
  }


  builtin_raft_n_sites {
    label: "Initial Number of Sites"
    desc:
      """~
      Number of storage sites that need to share responsibility over the set of storage shards.
      In this context, sites are EMQX nodes with message durability enabled.
      Please note that it takes effect only during the initialization of the durable storage database.
      During this phase at least that many sites should come online to distribute shards between them, otherwise message storage will be unavailable until then.
      After the initialization is complete, sites may be offline, which will affect availability depending on the number of offline sites and replication factor.~"""
  }

  builtin_raft_max_retries {
    label: "Maximum Retries"
    desc:
      """~
      Maximum number of retry attempts when encountering a recoverable intermittent failure.~"""
  }

  builtin_raft_retry_interval {
    label: "Retry Interval"
    desc:
      """~
      Sleep time between retry attempts.~"""
  }

  // Storage layouts:

  builtin_layout {
    label: "Storage Layout"
    desc:
      """~
      Storage layout is a method of arranging messages from various topics and clients on disc.

      Depending on the type of workload and the topic structure, different types of strategies for storing the data can be employed to maximize efficiency of reading messages from the durable storage.~"""
  }

  layout_builtin_wildcard_optimized_type {
    label: "Layout Type"
    desc:
      """~
      Wildcard-optimized layout type.~"""
  }

  layout_builtin_wildcard_optimized {
    label: "Wildcard-optimized Storage Layout"
    desc:
      """~
      _Wildcard-optimized_ layout is designed to maximize the throughput of wildcard subscriptions covering large numbers of topics.

      For example, it can handle scenarios where a very large number of clients publish data to the topics containing their client ID, such as: `sensor/%device-version%/%clientid%/temperature`, `sensor/%device-version%/%clientid%/pressure`, etc.
      This layout will automatically group such topics into a single stream, so a client subscribing to a topic filter containing wildcards (such as `sensor/+/+/temperature`) will be able to consume messages published by all devices as a single batch.

      This layout is efficient for non-wildcard subscriptions as well.~"""
  }

  lts_wildcard_thresholds {
    label: "Wildcard Thresholds"
    desc:
      """~
      Array of wildcard thresholds for different topic levels.

      EMQX can automatically group similar topic together to increase efficiency of message storage and retrieval.

      To understand the meaning of this configuration one can think of MQTT topics as a tree.
      When the number of children of a particular tree node reaches the threshold, EMQX performs the grouping.

      Adjusting this parameter helps when the topic structure is known in advance.
      For example, in the situations where multiple clients publish messages to a topic containing their client ID,
      such as `sensor/<clientid>/temperature/<room>`.
      Since `<clientid>` and `<room>` parts of the topic are variable, one can lower the wildcard threshold for topic levels 2 and 4 by setting this parameter to `[100, 0, 100, 0, 100]`.

      Each element of the array corresponds to a particular level of the topic.

      The last element of the array corresponds to all remaining levels as well.

      The array can't be empty.~"""
  }

  db_groups_root {
    label: "Database Groups"
    desc:
      """~
      A map of database group configurations.

      Database groups are used to share certain resources between the durable storage databases.
      For example, `storage_quota` parameter sets total disk usage allowance for all durable storages assigned to the group."""
  }

  db_group_record {
    label: "Database Group"
    desc: "Settings related to groups of durable storages using built-in backend."
  }

  db_group_name {
    label: "Group Name"
    desc: "Unique identifier of the group"
  }

  db_group_backend {
    label: "Group Backend"
    desc: "Backend used by all databases assigned to the group."
  }

  db_group_storage_quota {
    label: "Storage Quota"
    desc:
      """~
      Total disk space allowance for all durable storages assigned to the group.

      When the total size of durable storages exceeds the quota,
      attempts to write new data will fail until some disk space is freed.

      Built-in backends treat this value as a soft quota:
      disk space utilization may increase even as data is deleted from the database,
      until the database is compacted.

      In case of `builtin_raft` backend the quota check is performed by the leader node.
      Only shards that have local replicas are counted towards the quota."""
  }

  db_group_write_buffer_size {
    label: "Write Buffer Size"
    desc: "Maximum total RAM allocated for the DB write buffers."
  }
}
