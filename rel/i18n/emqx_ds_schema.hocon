emqx_ds_schema {
  // Databases:

  messages {
    label: "MQTT Message Storage"
    desc:
      """~
      Configuration of the durable storage for MQTT messages.~"""
  }

  sessions {
    label: "MQTT Session Storage"
    desc:
      """~
      Configuration of the durable storage containing state and metadata of the durable sessions.~"""
  }

  timers {
     label: "Durable Timers Storage"
     desc:
       """~
       Configuration of the durable storage containing timers.~"""
  }

  shared_subs {
     label: "Durable Shared Subscriptions"
     desc:
       """~
       Configuration of the durable storage containing metadata and topic replay progress by shared subscriptions.~"""
  }

  mq_states {
      label: "Message Queue states"
      desc:
        """~
        Configuration of the durable storage containing Message Queue states."""
  }

  mq_messages {
      label: "Message Queue messages"
      desc:
        """~
        Configuration of the durable storage containing Message Queue messages."""
  }

  streams_messages {
      label: "Message Stream messages"
      desc:
        """~
        Configuration of the durable storage containing Message Stream messages."""
  }

  // Backends types:

  backend_type {
    label: "Backend type"
    desc:
      """~
      Durable storage backend.

      Please note that it's impossible to change the backend of existing durable storages without losing the data.~"""
  }

  builtin_raft {
    label: "Builtin Raft"
    desc:
      """~
      An embedded durable storage backend with support for data replication.

      This backend is intended for use in clustered EMQX deployments.
      It replicates data for each durable storage shard on several EMQX nodes.~"""
  }

  builtin_local {
    label: "Builtin Local"
    desc:
      """~
      An embedded durable storage backend without network capabilities.

      This backend doesn't support clustering, but it has lower overhead than `builtin_raft`.
      It's recommended for use in strictly single-node deployments.~"""
  }

  // Common options

  builtin_n_shards {
    label: "Number of Shards"
    desc:
      """~
      The durable storage partitions data into shards.
      This configuration parameter defines the number of shards.
      Please note that it takes effect only during the initialization of the durable storage database.
      Changing this configuration parameter after the database has been already created won't take any effect.~"""
  }

  db_group {
    label: "Database Group"
    desc:
      """~
      Assign the durable storage to a database group with the given name.
      Certain resources (such as disk usage quota) will be shared among all members of the group.

      Backend of the database group should match that of the storage.~"""
  }


  // Common options of builtin backends:

  builtin_data_dir {
    label: "Database Location"
    desc:
      """~
      Root directory for the data related to built-in storage backends.

      By default, it is equal to `node.data_dir/ds/`.~"""
  }

  // OTX

  optimistic_transaction {
    label: "Transaction"
    desc:
      """~
      Transaction settings for built-in durable storage backends.~"""
  }

  builtin_optimistic_transaction {
    label: "Transaction Settings"
    desc: "Transaction settings."
  }

  otx_flush_interval {
    label: "Transaction Flush Interval"
    desc:
      """~
      Specifies the maximum time operations may linger in the buffer before they are committed to the storage.
      Larger values of the parameter may improve the throughput while negatively affecting the latency.~"""
  }

  otx_idle_flush_interval {
    label: "Idle Transaction Flush Interval"
    desc:
      """~
      If shard doesn't receive new transactions within this period, the buffer is flushed early.~"""
  }

  otx_max_pending {
    label: "Maximum Pending Operations"
    desc:
      """~
      Specifies the maximum number of operations that can be held in the buffer.
      When this value is exceeded, the buffer is flushed regardless of other factors.~"""
  }

  otx_conflict_window {
    label: "Conflict Tracking Window"
    desc:
      """~
      Built-in durable storage backends track recent updates over a period of time known as the conflict tracking window.

      Transactions that started earlier than the beginning of the window are automatically rejected.
      So, effectively, this parameter limits the time the transactions can run.

      Higher values reduce the risk of rejecting transactions due to long run time, but may increase RAM demands.

      This value should be greater than the flush interval.~"""
  }

  // RocksDB tuning:

  rocksdb_options {
    label: "RocksDB Options"
    desc:
      """~
      Options concerning behavior of RocksDB databases used by the built-in durable storage backends.~"""
  }

  rocksdb_cache_size {
    label: "RocksDB Cache Size"
    desc:
      """~
      Size of the RocksDB block cache.

      Note that increasing this value may not necessarily reduce I/O, since the compressed data may reside in the OS's page cache, but it can reduce CPU load at the expense of memory usage.

      Each shard of the database has its own cache.~"""
  }

  rocksdb_write_buffer_size {
    label: "RocksDB Write Buffer Size"
    desc:
      """~
      This parameter is deprecated and will be removed in a future release.
      It has been replaced with `durable_storage.db_groups.write_buffer_size` parameter,
      which has similar semantics, but is an aggregate for all shards.
      .~"""
  }

  rocksdb_max_open_files {
    label: "Maximum Open Files"
    desc:
      """~
      Maximum number of open file descriptors per shard.~"""
  }

  rocksdb_allow_fallocate {
    label: "Allow File Space Preallocation"
    desc:
      """~
      Allow RocksDB to perform file space preallocation through `fallocate`, if supported
      by the OS and the file system.
      File space preallocation is used to increase the file write/append performance at the
      cost of considerable disk usage per each shard, especially for an empty or sparsely
      populated DB.

      If enabled, make sure that the data directory resides on the volume with enough free
      space, e.g. at least several gigabytes.
      
      If the data directory is using `btrfs` file system, keeping it disabled is recommended.
      On `btrfs` the extra allocated space cannot be freed. This could be significant for DS
      databases, where having a lot of shards on a single site is common.
      
      Refer to RocksDB C++ API documentation of `allow_fallocate` flag for further details.~"""
  }

  // Raft-specific settings

  builtin_raft_replication_factor {
    label: "Replication Factor"
    desc:
      """~
      Number of identical replicas each shard should have.
      Increasing this number improves durability and availability at the expense of greater resource consumption.
      Quorum of replicas is needed to be healthy for the replication to work, hence an odd number of replicas is a good pick in general.
      Please note that it takes effect only during the initialization of the durable storage database.
      Changing this configuration parameter after the database has been already created won't take any effect.~"""
  }


  builtin_raft_n_sites {
    label: "Initial Number of Sites"
    desc:
      """~
      Number of storage sites that need to share responsibility over the set of storage shards.
      In this context, sites are EMQX nodes with message durability enabled.
      Please note that it takes effect only during the initialization of the durable storage database.
      During this phase at least that many sites should come online to distribute shards between them, otherwise message storage will be unavailable until then.
      After the initialization is complete, sites may be offline, which will affect availability depending on the number of offline sites and replication factor.~"""
  }

  builtin_raft_max_retries {
    label: "Maximum Retries"
    desc:
      """~
      Maximum number of retry attempts when encountering a recoverable intermittent failure.~"""
  }

  builtin_raft_retry_interval {
    label: "Retry Interval"
    desc:
      """~
      Sleep time between retry attempts.~"""
  }

  // Storage layouts:

  builtin_layout {
    label: "Storage Layout"
    desc:
      """~
      Storage layout is a method of arranging messages from various topics and clients on disc.

      Depending on the type of workload and the topic structure, different types of strategies for storing the data can be employed to maximize efficiency of reading messages from the durable storage.~"""
  }

  layout_builtin_wildcard_optimized_type {
    label: "Layout Type"
    desc:
      """~
      Wildcard-optimized layout type.~"""
  }

  layout_builtin_wildcard_optimized {
    label: "Wildcard-optimized Storage Layout"
    desc:
      """~
      _Wildcard-optimized_ layout is designed to maximize the throughput of wildcard subscriptions covering large numbers of topics.

      For example, it can handle scenarios where a very large number of clients publish data to the topics containing their client ID, such as: `sensor/%device-version%/%clientid%/temperature`, `sensor/%device-version%/%clientid%/pressure`, etc.
      This layout will automatically group such topics into a single stream, so a client subscribing to a topic filter containing wildcards (such as `sensor/+/+/temperature`) will be able to consume messages published by all devices as a single batch.

      This layout is efficient for non-wildcard subscriptions as well.~"""
  }

  lts_wildcard_thresholds {
    label: "Wildcard Thresholds"
    desc:
      """~
      Array of wildcard thresholds for different topic levels.

      EMQX can automatically group similar topic together to increase efficiency of message storage and retrieval.

      To understand the meaning of this configuration one can think of MQTT topics as a tree.
      When the number of children of a particular tree node reaches the threshold, EMQX performs the grouping.

      Adjusting this parameter helps when the topic structure is known in advance.
      For example, in the situations where multiple clients publish messages to a topic containing their client ID,
      such as `sensor/<clientid>/temperature/<room>`.
      Since `<clientid>` and `<room>` parts of the topic are variable, one can lower the wildcard threshold for topic levels 2 and 4 by setting this parameter to `[100, 0, 100, 0, 100]`.

      Each element of the array corresponds to a particular level of the topic.

      The last element of the array corresponds to all remaining levels as well.

      The array can't be empty.~"""
  }

  db_groups_root {
    label: "Database Groups"
    desc:
      """~
      A map of database group configurations.

      Database groups are used to share certain resources between the durable storage databases.
      For example, `storage_quota` parameter sets total disk usage allowance for all durable storages assigned to the group."""
  }

  db_group_record {
    label: "Database Group"
    desc: "Settings related to groups of durable storages using built-in backend."
  }

  db_group_name {
    label: "Group Name"
    desc: "Unique identifier of the group"
  }

  db_group_backend {
    label: "Group Backend"
    desc: "Backend used by all databases assigned to the group."
  }

  db_group_storage_quota {
    label: "Storage Quota"
    desc:
      """~
      Total disk space allowance for all durable storages assigned to the group.

      When the total size of durable storages exceeds the quota,
      attempts to write new data will fail until some disk space is freed.

      Built-in backends treat this value as a soft quota:
      disk space utilization may increase even as data is deleted from the database,
      until the database is compacted.

      In case of `builtin_raft` backend the quota check is performed by the leader node.
      Only shards that have local replicas are counted towards the quota.

      `db_storage_quota_exceeded:<DB>` alarm is raised when the quota is exceeded.

      The consequences of exceeding the quota depend on the DB:
      - `messages`: durable sessions cannot receive new messages.
        Mitigations:
        + Reduce `durable_sessions.message_retention_period` configuration parameter.
        + Use `mqtt.subscription_max_qos_rules` to set subscription QoS for certain topics to 0.
          Message delivery to QoS 0 subscriptions does not use disk space.
      - `sessions`: durable sessions are disconnected from the broker.
        Mitigations:
        + Kick offline sessions.
        + Limit the maximum number of sessions.~"""
  }

  db_group_write_buffer_size {
    label: "Write Buffer Size"
    desc: "Maximum total RAM allocated for the DB write buffers."
  }

  db_group_rocksdb_threads {
    label: "Size of RocksDB Thread Pool"
    desc:
      """~
      Number of RocksDB background threads per durable storage group.

      RocksDB uses two thread pools: high and low priority.
      The former is used for dumping memtables to disk,
      and the latter is used for compaction.

      By default, size of both pools is set to the number of dirty IO Schedulers.~"""
  }
}
